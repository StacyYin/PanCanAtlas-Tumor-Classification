{"cells":[{"cell_type":"markdown","source":["##Summary\n\n\n* Import Libraries\n* Load and Preview Data from AWS Data Lake\n* Data Wrangling\n  * String Mapping\n  * Use Chi-Square to Select Top 1000 Features\n  * Reshape and split the data \n* Deep Learning Architecture\n  * Set the Hyperparameters\n  * Model Construction\n\t* Learning Rate / Adam Decay Tuning Based on Gaussian Process\n\t* Bayesian Optimization\n\t* Best Model for Training\n* Model Evaluation and Model Fitting"],"metadata":{}},{"cell_type":"markdown","source":["## Potential Application of Deep Learning using TCGA\n* ### Identify potential tissue of origin of unknown cancer types by molecular profiles\n* ### Predict tissue of origin of metastatic cancer"],"metadata":{}},{"cell_type":"markdown","source":["Large-scale cancer genomics data often imposes great challenges in terms of computational algorithms. The high dimensioanl dataset is suitable for applying a deep learning algorithm. Given the features and labels, models can be trained to classify future samples based on similar gene expression."],"metadata":{}},{"cell_type":"markdown","source":["## RNA-Seq (HiSeq) PANCAN Data Set"],"metadata":{}},{"cell_type":"markdown","source":["Data Abstract:\n\nThis collection of data is part of the RNA-Seq (HiSeq) PANCAN data set. It is an extraction of gene expressions of patients having different types of tumor: BRCA, KIRC, COAD, LUAD and PRAD. The data source is UCI Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq"],"metadata":{}},{"cell_type":"markdown","source":["## Import Libraries"],"metadata":{}},{"cell_type":"markdown","source":["Keras API will be used to conduct multilayer deep learning models. Scikit-Optimize is used to optimize the best parameters. And plotly is imported to do vivid and interactive data visualization."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport plotly.graph_objs as go\nfrom collections import Counter\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split\nfrom plotly.subplots import make_subplots\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import LSTM, Embedding, Flatten\nfrom keras import optimizers, regularizers\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\nimport skopt\nfrom skopt import gbrt_minimize, gp_minimize\nfrom skopt.utils import use_named_args\nfrom skopt.space import Real, Categorical, Integer  \n\nimport tensorflow\nfrom tensorflow.python.keras import backend as K"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Using TensorFlow backend.\n/databricks/python/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n  from collections import Mapping, MutableMapping\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["## Load and Preview Data from AWS Data Lake"],"metadata":{}},{"cell_type":"markdown","source":["*Data Loading*"],"metadata":{}},{"cell_type":"markdown","source":["Datasets were stored in our Gold Bucket, AWS Data Lake. Feature data and label data were separately stored."],"metadata":{}},{"cell_type":"code","source":["# Get access to AWS data lake\nACCESS_KEY= os.environ['AWS_ACCESS_KEY']\nENCODED_SECRET_KEY = os.environ['AWS_SECRET_ACCESS_KEY'].replace('/', '%2F')\n\nGOLD_BUCKET = 'oculadata-gold'\nMOUNT_GOLD = '/mnt/gold'\n\nmounted_paths = map(lambda m: m.mountPoint, dbutils.fs.mounts())\n\n# Mount the data\nif MOUNT_GOLD not in mounted_paths:\n  dbutils.fs.mount(f's3a://{ACCESS_KEY}:{ENCODED_SECRET_KEY}@{GOLD_BUCKET}', MOUNT_GOLD)\n  \nprint(dbutils.fs.mounts())\n\nproject = 'PanCanAtlas'\nfilepath = f'{MOUNT_GOLD}/{project}'\n\ninfer_schema = 'true'\nif_header = 'true'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[MountInfo(mountPoint=&#39;/databricks-datasets&#39;, source=&#39;databricks-datasets&#39;, encryptionType=&#39;sse-s3&#39;), MountInfo(mountPoint=&#39;/mnt/gold&#39;, source=&#39;s3a://oculadata-gold&#39;, encryptionType=&#39;&#39;), MountInfo(mountPoint=&#39;/databricks/mlflow-tracking&#39;, source=&#39;databricks/mlflow-tracking&#39;, encryptionType=&#39;sse-s3&#39;), MountInfo(mountPoint=&#39;/databricks-results&#39;, source=&#39;databricks-results&#39;, encryptionType=&#39;sse-s3&#39;), MountInfo(mountPoint=&#39;/mnt/mount-oculadatalanding&#39;, source=&#39;s3a://oculadatalanding&#39;, encryptionType=&#39;&#39;), MountInfo(mountPoint=&#39;/databricks/mlflow-registry&#39;, source=&#39;databricks/mlflow-registry&#39;, encryptionType=&#39;sse-s3&#39;), MountInfo(mountPoint=&#39;/&#39;, source=&#39;DatabricksRoot&#39;, encryptionType=&#39;sse-s3&#39;)]\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# get the feature data\ntablename = 'Sameple_Cancer_data'\nfile_location = f'dbfs:{filepath}/{tablename}/'\nfile_type = 'delta'\n\nr_data = spark.read.option(\"maxColumns\", 25000).format(file_type)\\\n        .option(\"header\", if_header)\\\n        .option(\"inferSchema\", infer_schema)\\\n        .load(file_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["r_data.limit(10).toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_c0</th>\n      <th>gene_0</th>\n      <th>gene_1</th>\n      <th>gene_2</th>\n      <th>gene_3</th>\n      <th>gene_4</th>\n      <th>gene_5</th>\n      <th>gene_6</th>\n      <th>gene_7</th>\n      <th>gene_8</th>\n      <th>gene_9</th>\n      <th>gene_10</th>\n      <th>gene_11</th>\n      <th>gene_12</th>\n      <th>gene_13</th>\n      <th>gene_14</th>\n      <th>gene_15</th>\n      <th>gene_16</th>\n      <th>gene_17</th>\n      <th>gene_18</th>\n      <th>gene_19</th>\n      <th>gene_20</th>\n      <th>gene_21</th>\n      <th>gene_22</th>\n      <th>gene_23</th>\n      <th>gene_24</th>\n      <th>gene_25</th>\n      <th>gene_26</th>\n      <th>gene_27</th>\n      <th>gene_28</th>\n      <th>gene_29</th>\n      <th>gene_30</th>\n      <th>gene_31</th>\n      <th>gene_32</th>\n      <th>gene_33</th>\n      <th>gene_34</th>\n      <th>gene_35</th>\n      <th>gene_36</th>\n      <th>gene_37</th>\n      <th>gene_38</th>\n      <th>...</th>\n      <th>gene_20491</th>\n      <th>gene_20492</th>\n      <th>gene_20493</th>\n      <th>gene_20494</th>\n      <th>gene_20495</th>\n      <th>gene_20496</th>\n      <th>gene_20497</th>\n      <th>gene_20498</th>\n      <th>gene_20499</th>\n      <th>gene_20500</th>\n      <th>gene_20501</th>\n      <th>gene_20502</th>\n      <th>gene_20503</th>\n      <th>gene_20504</th>\n      <th>gene_20505</th>\n      <th>gene_20506</th>\n      <th>gene_20507</th>\n      <th>gene_20508</th>\n      <th>gene_20509</th>\n      <th>gene_20510</th>\n      <th>gene_20511</th>\n      <th>gene_20512</th>\n      <th>gene_20513</th>\n      <th>gene_20514</th>\n      <th>gene_20515</th>\n      <th>gene_20516</th>\n      <th>gene_20517</th>\n      <th>gene_20518</th>\n      <th>gene_20519</th>\n      <th>gene_20520</th>\n      <th>gene_20521</th>\n      <th>gene_20522</th>\n      <th>gene_20523</th>\n      <th>gene_20524</th>\n      <th>gene_20525</th>\n      <th>gene_20526</th>\n      <th>gene_20527</th>\n      <th>gene_20528</th>\n      <th>gene_20529</th>\n      <th>gene_20530</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sample_0</td>\n      <td>0.0</td>\n      <td>2.017209</td>\n      <td>3.265527</td>\n      <td>5.478487</td>\n      <td>10.431999</td>\n      <td>0.0</td>\n      <td>7.175175</td>\n      <td>0.591871</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.591871</td>\n      <td>1.334282</td>\n      <td>2.015391</td>\n      <td>0.591871</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.591871</td>\n      <td>5.619994</td>\n      <td>1.334282</td>\n      <td>0.000000</td>\n      <td>9.796088</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.598651</td>\n      <td>7.215116</td>\n      <td>10.839070</td>\n      <td>6.620204</td>\n      <td>9.513538</td>\n      <td>0.000000</td>\n      <td>4.063658</td>\n      <td>7.764805</td>\n      <td>4.747656</td>\n      <td>13.714396</td>\n      <td>10.034496</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>9.833458</td>\n      <td>...</td>\n      <td>9.370304</td>\n      <td>10.362393</td>\n      <td>5.589928</td>\n      <td>8.141964</td>\n      <td>0.000000</td>\n      <td>2.736583</td>\n      <td>7.037152</td>\n      <td>7.123480</td>\n      <td>10.967399</td>\n      <td>5.902800</td>\n      <td>3.719370</td>\n      <td>7.203554</td>\n      <td>6.042557</td>\n      <td>2.602077</td>\n      <td>7.425526</td>\n      <td>7.846957</td>\n      <td>2.824951</td>\n      <td>6.239396</td>\n      <td>0.000000</td>\n      <td>8.469593</td>\n      <td>0.0</td>\n      <td>6.535978</td>\n      <td>6.968701</td>\n      <td>7.128881</td>\n      <td>7.175175</td>\n      <td>9.249369</td>\n      <td>7.025970</td>\n      <td>8.045563</td>\n      <td>7.475709</td>\n      <td>7.205236</td>\n      <td>4.926711</td>\n      <td>8.210257</td>\n      <td>9.723516</td>\n      <td>7.220030</td>\n      <td>9.119813</td>\n      <td>12.003135</td>\n      <td>9.650743</td>\n      <td>8.921326</td>\n      <td>5.286759</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sample_1</td>\n      <td>0.0</td>\n      <td>0.592732</td>\n      <td>1.588421</td>\n      <td>7.586157</td>\n      <td>9.623011</td>\n      <td>0.0</td>\n      <td>6.816049</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.587845</td>\n      <td>2.466601</td>\n      <td>1.004394</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>11.055208</td>\n      <td>3.562621</td>\n      <td>0.000000</td>\n      <td>10.070470</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.949812</td>\n      <td>8.522476</td>\n      <td>1.174790</td>\n      <td>4.926991</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.819832</td>\n      <td>1.327170</td>\n      <td>13.286240</td>\n      <td>6.663316</td>\n      <td>0.587845</td>\n      <td>0.0</td>\n      <td>9.533302</td>\n      <td>...</td>\n      <td>8.882967</td>\n      <td>9.898199</td>\n      <td>7.069401</td>\n      <td>7.186134</td>\n      <td>0.000000</td>\n      <td>3.134993</td>\n      <td>6.648930</td>\n      <td>6.715701</td>\n      <td>9.536238</td>\n      <td>1.004394</td>\n      <td>5.555482</td>\n      <td>8.029260</td>\n      <td>6.366219</td>\n      <td>0.811142</td>\n      <td>7.991732</td>\n      <td>7.161001</td>\n      <td>0.000000</td>\n      <td>4.708877</td>\n      <td>0.811142</td>\n      <td>8.451689</td>\n      <td>0.0</td>\n      <td>7.242336</td>\n      <td>8.046284</td>\n      <td>6.047558</td>\n      <td>8.572901</td>\n      <td>7.549030</td>\n      <td>7.019935</td>\n      <td>9.458940</td>\n      <td>9.190867</td>\n      <td>10.639259</td>\n      <td>4.593372</td>\n      <td>7.323865</td>\n      <td>9.740931</td>\n      <td>6.256586</td>\n      <td>8.381612</td>\n      <td>12.674552</td>\n      <td>10.517059</td>\n      <td>9.397854</td>\n      <td>2.094168</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sample_2</td>\n      <td>0.0</td>\n      <td>3.511759</td>\n      <td>4.327199</td>\n      <td>6.881787</td>\n      <td>9.870730</td>\n      <td>0.0</td>\n      <td>6.972130</td>\n      <td>0.452595</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.452595</td>\n      <td>1.981122</td>\n      <td>1.074163</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.683023</td>\n      <td>8.210248</td>\n      <td>4.195285</td>\n      <td>3.660427</td>\n      <td>8.970920</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.796598</td>\n      <td>6.096650</td>\n      <td>9.861616</td>\n      <td>7.680507</td>\n      <td>3.119439</td>\n      <td>0.000000</td>\n      <td>0.452595</td>\n      <td>7.899526</td>\n      <td>0.000000</td>\n      <td>10.731098</td>\n      <td>6.967883</td>\n      <td>0.452595</td>\n      <td>0.0</td>\n      <td>9.646323</td>\n      <td>...</td>\n      <td>10.355637</td>\n      <td>10.423274</td>\n      <td>5.170201</td>\n      <td>6.194260</td>\n      <td>0.000000</td>\n      <td>3.677147</td>\n      <td>6.271990</td>\n      <td>7.089816</td>\n      <td>9.675220</td>\n      <td>0.000000</td>\n      <td>4.224017</td>\n      <td>8.020402</td>\n      <td>6.967883</td>\n      <td>5.014445</td>\n      <td>8.400038</td>\n      <td>7.527555</td>\n      <td>0.000000</td>\n      <td>4.997902</td>\n      <td>0.796598</td>\n      <td>7.761132</td>\n      <td>0.0</td>\n      <td>6.820460</td>\n      <td>8.048983</td>\n      <td>6.661493</td>\n      <td>7.716332</td>\n      <td>6.745802</td>\n      <td>7.524667</td>\n      <td>8.602350</td>\n      <td>9.036654</td>\n      <td>10.336027</td>\n      <td>5.125213</td>\n      <td>8.127123</td>\n      <td>10.908640</td>\n      <td>5.401607</td>\n      <td>9.911597</td>\n      <td>9.045255</td>\n      <td>9.788359</td>\n      <td>10.090470</td>\n      <td>1.683023</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sample_3</td>\n      <td>0.0</td>\n      <td>3.663618</td>\n      <td>4.507649</td>\n      <td>6.659068</td>\n      <td>10.196184</td>\n      <td>0.0</td>\n      <td>7.843375</td>\n      <td>0.434882</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.434882</td>\n      <td>2.874246</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.267356</td>\n      <td>8.306317</td>\n      <td>3.573556</td>\n      <td>0.000000</td>\n      <td>8.524616</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.913761</td>\n      <td>9.511573</td>\n      <td>6.469165</td>\n      <td>7.029895</td>\n      <td>0.000000</td>\n      <td>1.267356</td>\n      <td>6.800641</td>\n      <td>7.742714</td>\n      <td>12.659474</td>\n      <td>8.299890</td>\n      <td>0.768587</td>\n      <td>0.0</td>\n      <td>9.670731</td>\n      <td>...</td>\n      <td>10.074382</td>\n      <td>9.918261</td>\n      <td>7.117924</td>\n      <td>7.196145</td>\n      <td>0.434882</td>\n      <td>3.609755</td>\n      <td>8.896696</td>\n      <td>7.577096</td>\n      <td>10.731446</td>\n      <td>5.075383</td>\n      <td>2.175652</td>\n      <td>7.675435</td>\n      <td>6.840816</td>\n      <td>6.233192</td>\n      <td>8.899886</td>\n      <td>8.319085</td>\n      <td>1.791814</td>\n      <td>5.661134</td>\n      <td>1.464093</td>\n      <td>8.625727</td>\n      <td>0.0</td>\n      <td>7.420095</td>\n      <td>7.784746</td>\n      <td>7.613915</td>\n      <td>8.963286</td>\n      <td>7.744699</td>\n      <td>7.924997</td>\n      <td>8.981473</td>\n      <td>8.665592</td>\n      <td>9.194823</td>\n      <td>6.076566</td>\n      <td>8.792959</td>\n      <td>10.141520</td>\n      <td>8.942805</td>\n      <td>9.601208</td>\n      <td>11.392682</td>\n      <td>9.694814</td>\n      <td>9.684365</td>\n      <td>3.292001</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sample_4</td>\n      <td>0.0</td>\n      <td>2.655741</td>\n      <td>2.821547</td>\n      <td>6.539454</td>\n      <td>9.738265</td>\n      <td>0.0</td>\n      <td>6.566967</td>\n      <td>0.360982</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.275841</td>\n      <td>2.141204</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.889707</td>\n      <td>10.149150</td>\n      <td>2.967630</td>\n      <td>0.000000</td>\n      <td>8.047238</td>\n      <td>0.0</td>\n      <td>1.435949</td>\n      <td>0.000000</td>\n      <td>1.942120</td>\n      <td>8.821535</td>\n      <td>5.861429</td>\n      <td>7.755709</td>\n      <td>0.000000</td>\n      <td>0.649386</td>\n      <td>5.570241</td>\n      <td>2.612801</td>\n      <td>13.556734</td>\n      <td>8.004754</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>9.587569</td>\n      <td>...</td>\n      <td>10.129154</td>\n      <td>10.062303</td>\n      <td>6.911620</td>\n      <td>7.855149</td>\n      <td>0.360982</td>\n      <td>3.655810</td>\n      <td>7.255520</td>\n      <td>7.292607</td>\n      <td>10.779793</td>\n      <td>3.954001</td>\n      <td>6.991148</td>\n      <td>8.153248</td>\n      <td>7.508444</td>\n      <td>4.586531</td>\n      <td>9.152227</td>\n      <td>8.227717</td>\n      <td>0.360982</td>\n      <td>6.227104</td>\n      <td>0.649386</td>\n      <td>8.151879</td>\n      <td>0.0</td>\n      <td>6.558289</td>\n      <td>8.673708</td>\n      <td>6.505099</td>\n      <td>8.948989</td>\n      <td>7.010366</td>\n      <td>7.364056</td>\n      <td>8.950646</td>\n      <td>8.233366</td>\n      <td>9.298775</td>\n      <td>5.996032</td>\n      <td>8.891425</td>\n      <td>10.373790</td>\n      <td>7.181162</td>\n      <td>9.846910</td>\n      <td>11.922439</td>\n      <td>9.217749</td>\n      <td>9.461191</td>\n      <td>5.110372</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sample_5</td>\n      <td>0.0</td>\n      <td>3.467853</td>\n      <td>3.581918</td>\n      <td>6.620243</td>\n      <td>9.706829</td>\n      <td>0.0</td>\n      <td>7.758510</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.515410</td>\n      <td>0.515410</td>\n      <td>2.516797</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.894294</td>\n      <td>0.894294</td>\n      <td>6.842765</td>\n      <td>2.809661</td>\n      <td>4.002901</td>\n      <td>7.663935</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.194150</td>\n      <td>0.894294</td>\n      <td>9.628387</td>\n      <td>7.220446</td>\n      <td>7.012759</td>\n      <td>4.936704</td>\n      <td>1.442280</td>\n      <td>8.002236</td>\n      <td>2.002018</td>\n      <td>14.021153</td>\n      <td>9.126059</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>9.962887</td>\n      <td>...</td>\n      <td>9.816689</td>\n      <td>10.207576</td>\n      <td>6.640314</td>\n      <td>7.761365</td>\n      <td>0.894294</td>\n      <td>4.267618</td>\n      <td>7.844574</td>\n      <td>7.269248</td>\n      <td>10.893810</td>\n      <td>5.161037</td>\n      <td>3.749717</td>\n      <td>8.251047</td>\n      <td>6.981579</td>\n      <td>5.761330</td>\n      <td>8.431932</td>\n      <td>8.801725</td>\n      <td>0.515410</td>\n      <td>7.471439</td>\n      <td>2.895400</td>\n      <td>7.953539</td>\n      <td>0.0</td>\n      <td>7.067208</td>\n      <td>8.077959</td>\n      <td>7.973393</td>\n      <td>8.380816</td>\n      <td>7.792270</td>\n      <td>7.015482</td>\n      <td>8.664963</td>\n      <td>8.344820</td>\n      <td>7.831035</td>\n      <td>5.726657</td>\n      <td>8.602588</td>\n      <td>9.928339</td>\n      <td>6.096154</td>\n      <td>9.816001</td>\n      <td>11.556995</td>\n      <td>9.244150</td>\n      <td>9.836473</td>\n      <td>5.355133</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>sample_6</td>\n      <td>0.0</td>\n      <td>1.224966</td>\n      <td>1.691177</td>\n      <td>6.572007</td>\n      <td>9.640511</td>\n      <td>0.0</td>\n      <td>6.754888</td>\n      <td>0.531868</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>3.173927</td>\n      <td>1.476796</td>\n      <td>3.023841</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>9.466878</td>\n      <td>7.424200</td>\n      <td>1.224966</td>\n      <td>0.000000</td>\n      <td>9.973640</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.508977</td>\n      <td>4.485420</td>\n      <td>8.633177</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.899400</td>\n      <td>0.000000</td>\n      <td>15.597753</td>\n      <td>10.909466</td>\n      <td>1.476796</td>\n      <td>0.0</td>\n      <td>9.145458</td>\n      <td>...</td>\n      <td>9.985102</td>\n      <td>9.971012</td>\n      <td>4.456418</td>\n      <td>8.152533</td>\n      <td>0.919683</td>\n      <td>4.680133</td>\n      <td>7.757137</td>\n      <td>6.724732</td>\n      <td>11.044708</td>\n      <td>4.334196</td>\n      <td>5.314236</td>\n      <td>8.203343</td>\n      <td>6.557330</td>\n      <td>4.200646</td>\n      <td>8.370317</td>\n      <td>7.863146</td>\n      <td>3.434068</td>\n      <td>6.167663</td>\n      <td>0.919683</td>\n      <td>7.964479</td>\n      <td>0.0</td>\n      <td>6.501632</td>\n      <td>9.371053</td>\n      <td>6.271224</td>\n      <td>9.115624</td>\n      <td>8.393017</td>\n      <td>6.724787</td>\n      <td>7.507858</td>\n      <td>6.862712</td>\n      <td>6.830293</td>\n      <td>5.105904</td>\n      <td>7.927968</td>\n      <td>9.673966</td>\n      <td>1.877744</td>\n      <td>9.802692</td>\n      <td>13.256060</td>\n      <td>9.664486</td>\n      <td>9.244219</td>\n      <td>8.330912</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>sample_7</td>\n      <td>0.0</td>\n      <td>2.854853</td>\n      <td>1.750478</td>\n      <td>7.226720</td>\n      <td>9.758691</td>\n      <td>0.0</td>\n      <td>5.952103</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.441802</td>\n      <td>0.000000</td>\n      <td>2.405856</td>\n      <td>0.000000</td>\n      <td>0.779554</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.373431</td>\n      <td>2.304861</td>\n      <td>0.000000</td>\n      <td>8.922008</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.145527</td>\n      <td>9.406279</td>\n      <td>4.664710</td>\n      <td>5.705193</td>\n      <td>0.000000</td>\n      <td>6.454625</td>\n      <td>7.012714</td>\n      <td>5.145527</td>\n      <td>16.798586</td>\n      <td>10.113911</td>\n      <td>1.053042</td>\n      <td>0.0</td>\n      <td>9.349567</td>\n      <td>...</td>\n      <td>10.141175</td>\n      <td>9.962960</td>\n      <td>6.154326</td>\n      <td>7.728165</td>\n      <td>0.779554</td>\n      <td>4.797594</td>\n      <td>7.538228</td>\n      <td>6.418822</td>\n      <td>10.773593</td>\n      <td>2.304861</td>\n      <td>5.101036</td>\n      <td>8.043897</td>\n      <td>6.495300</td>\n      <td>5.085896</td>\n      <td>8.359666</td>\n      <td>7.691122</td>\n      <td>0.000000</td>\n      <td>5.516860</td>\n      <td>1.282855</td>\n      <td>7.849668</td>\n      <td>0.0</td>\n      <td>6.472199</td>\n      <td>7.653110</td>\n      <td>7.603471</td>\n      <td>8.468738</td>\n      <td>8.466199</td>\n      <td>7.110896</td>\n      <td>8.358093</td>\n      <td>7.422199</td>\n      <td>6.460507</td>\n      <td>5.297833</td>\n      <td>8.277092</td>\n      <td>9.599230</td>\n      <td>5.244290</td>\n      <td>9.994339</td>\n      <td>12.670377</td>\n      <td>9.987733</td>\n      <td>9.216872</td>\n      <td>6.551490</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sample_8</td>\n      <td>0.0</td>\n      <td>3.992125</td>\n      <td>2.772730</td>\n      <td>6.546692</td>\n      <td>10.488252</td>\n      <td>0.0</td>\n      <td>7.690222</td>\n      <td>0.352307</td>\n      <td>0.0</td>\n      <td>4.067604</td>\n      <td>1.411318</td>\n      <td>1.252839</td>\n      <td>2.579977</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.635336</td>\n      <td>10.147625</td>\n      <td>4.287908</td>\n      <td>4.773828</td>\n      <td>8.343976</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.080374</td>\n      <td>7.600537</td>\n      <td>2.824931</td>\n      <td>4.934233</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.244617</td>\n      <td>9.464329</td>\n      <td>13.531004</td>\n      <td>5.944411</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>8.542196</td>\n      <td>...</td>\n      <td>11.350210</td>\n      <td>10.845960</td>\n      <td>6.599748</td>\n      <td>7.967163</td>\n      <td>0.000000</td>\n      <td>7.629808</td>\n      <td>8.682851</td>\n      <td>7.812396</td>\n      <td>9.555722</td>\n      <td>1.554049</td>\n      <td>6.406288</td>\n      <td>8.114591</td>\n      <td>7.416570</td>\n      <td>3.082175</td>\n      <td>9.394889</td>\n      <td>7.888506</td>\n      <td>0.635336</td>\n      <td>6.865783</td>\n      <td>1.252839</td>\n      <td>7.504906</td>\n      <td>0.0</td>\n      <td>6.501863</td>\n      <td>8.488764</td>\n      <td>4.308193</td>\n      <td>8.290171</td>\n      <td>8.069799</td>\n      <td>7.987747</td>\n      <td>9.469967</td>\n      <td>9.595914</td>\n      <td>10.461326</td>\n      <td>6.721974</td>\n      <td>9.597533</td>\n      <td>9.763753</td>\n      <td>7.933278</td>\n      <td>10.952880</td>\n      <td>12.498919</td>\n      <td>10.389954</td>\n      <td>10.390255</td>\n      <td>7.828321</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sample_9</td>\n      <td>0.0</td>\n      <td>3.642494</td>\n      <td>4.423558</td>\n      <td>6.849511</td>\n      <td>9.464466</td>\n      <td>0.0</td>\n      <td>7.947216</td>\n      <td>0.724214</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.204141</td>\n      <td>2.296311</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.856780</td>\n      <td>1.204141</td>\n      <td>5.784391</td>\n      <td>6.020051</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.724214</td>\n      <td>9.871312</td>\n      <td>6.118104</td>\n      <td>6.284211</td>\n      <td>0.000000</td>\n      <td>5.070381</td>\n      <td>5.907184</td>\n      <td>0.000000</td>\n      <td>12.578644</td>\n      <td>8.285411</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>9.292131</td>\n      <td>...</td>\n      <td>10.713593</td>\n      <td>10.071543</td>\n      <td>5.945694</td>\n      <td>7.397144</td>\n      <td>0.000000</td>\n      <td>3.314972</td>\n      <td>8.712809</td>\n      <td>7.656704</td>\n      <td>10.816280</td>\n      <td>5.417103</td>\n      <td>2.635987</td>\n      <td>7.984350</td>\n      <td>7.346026</td>\n      <td>7.023422</td>\n      <td>8.855697</td>\n      <td>9.512906</td>\n      <td>0.000000</td>\n      <td>6.854881</td>\n      <td>1.204141</td>\n      <td>8.309331</td>\n      <td>0.0</td>\n      <td>7.023422</td>\n      <td>8.521294</td>\n      <td>8.318199</td>\n      <td>8.258024</td>\n      <td>7.832390</td>\n      <td>7.747220</td>\n      <td>8.629757</td>\n      <td>7.014802</td>\n      <td>8.267213</td>\n      <td>6.020051</td>\n      <td>8.712809</td>\n      <td>10.259096</td>\n      <td>6.131583</td>\n      <td>9.923582</td>\n      <td>11.144295</td>\n      <td>9.244851</td>\n      <td>9.484299</td>\n      <td>4.759151</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 20532 columns</p>\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# get the label data\ntablename = 'Sameple_Cancer_labels'\nfile_location = f'dbfs:{filepath}/{tablename}/'\n\nr_label = spark.read.format(file_type)\\\n                  .option(\"header\", if_header)\\\n                  .option(\"inferSchema\", infer_schema)\\\n                  .option(\"delimiter\", \",\")\\\n                  .load(file_location)\n\nr_label.limit(10).toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_c0</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sample_0</td>\n      <td>PRAD</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sample_1</td>\n      <td>LUAD</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sample_2</td>\n      <td>PRAD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sample_3</td>\n      <td>PRAD</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sample_4</td>\n      <td>BRCA</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>sample_5</td>\n      <td>PRAD</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>sample_6</td>\n      <td>KIRC</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>sample_7</td>\n      <td>PRAD</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>sample_8</td>\n      <td>BRCA</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sample_9</td>\n      <td>PRAD</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["## Data Wrangling"],"metadata":{}},{"cell_type":"markdown","source":["String data cannot be directly used in model training. So we need to map it to different dummy varibles, which can be distinguished by the algorithm."],"metadata":{}},{"cell_type":"markdown","source":["## String Mapping"],"metadata":{}},{"cell_type":"code","source":["# NULL value check\nr_label.filter('Class is null').show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-----+\n_c0|Class|\n+---+-----+\n+---+-----+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# category indexing with StringIndexer\nindexer = StringIndexer(inputCol = 'Class', outputCol = 'Class_Index') \nindexed_r_label = indexer.fit(r_label).transform(r_label)\n\n# OneHotEncodeer to convert into binary values\nencoder = OneHotEncoder(inputCol='Class_Index', outputCol='Class_Vec',dropLast = False)\nindexed_r_label = encoder.transform(indexed_r_label)\n\n# split into different columns\nindexed_r_label = indexed_r_label.select('Class_Vec').rdd.map(lambda x:x[0].toArray().tolist()).toDF()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["# show tranferred binary value\nindexed_r_label.limit(10).toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_1</th>\n      <th>_2</th>\n      <th>_3</th>\n      <th>_4</th>\n      <th>_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["## Use chi-square to select top 1000 features"],"metadata":{}},{"cell_type":"code","source":["# apply SelectKBest class to extract top 1000 best features\nbestfeatures = SelectKBest(score_func = chi2, k=1000)\nfit = bestfeatures.fit(r_data,indexed_r_label)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(r_data.columns) \n\n# concat the column name\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  \n\n# select top 1000 best features\nSelected_Index = featureScores.nlargest(1000,'Score').index\nS_Index = Selected_Index.toPandas().values.flatten().tolist()\n\nr_data_ext = r_data.select(*(r_data1.columns[i] for i in S_Index))\nr_data_ext.limit(10).toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>gene_9176</th>\n      <th>gene_9175</th>\n      <th>gene_15898</th>\n      <th>gene_220</th>\n      <th>gene_219</th>\n      <th>gene_15896</th>\n      <th>gene_18135</th>\n      <th>gene_15899</th>\n      <th>gene_12069</th>\n      <th>gene_15895</th>\n      <th>gene_13976</th>\n      <th>gene_16132</th>\n      <th>gene_12995</th>\n      <th>gene_3439</th>\n      <th>gene_19153</th>\n      <th>gene_5829</th>\n      <th>gene_450</th>\n      <th>gene_16169</th>\n      <th>gene_15591</th>\n      <th>gene_11903</th>\n      <th>gene_14114</th>\n      <th>gene_3737</th>\n      <th>gene_16259</th>\n      <th>gene_16130</th>\n      <th>gene_13639</th>\n      <th>gene_6937</th>\n      <th>gene_15589</th>\n      <th>gene_1858</th>\n      <th>gene_3813</th>\n      <th>gene_16105</th>\n      <th>gene_13818</th>\n      <th>gene_3524</th>\n      <th>gene_12881</th>\n      <th>gene_16156</th>\n      <th>gene_15894</th>\n      <th>gene_9232</th>\n      <th>gene_12848</th>\n      <th>gene_1510</th>\n      <th>gene_7965</th>\n      <th>gene_16392</th>\n      <th>...</th>\n      <th>gene_9924</th>\n      <th>gene_18900</th>\n      <th>gene_16248</th>\n      <th>gene_5361</th>\n      <th>gene_7040</th>\n      <th>gene_9168</th>\n      <th>gene_13308</th>\n      <th>gene_3138</th>\n      <th>gene_2260</th>\n      <th>gene_753</th>\n      <th>gene_15604</th>\n      <th>gene_5752</th>\n      <th>gene_9181</th>\n      <th>gene_8193</th>\n      <th>gene_7659</th>\n      <th>gene_11393</th>\n      <th>gene_6070</th>\n      <th>gene_11576</th>\n      <th>gene_1331</th>\n      <th>gene_17680</th>\n      <th>gene_6850</th>\n      <th>gene_8431</th>\n      <th>gene_1992</th>\n      <th>gene_10421</th>\n      <th>gene_10690</th>\n      <th>gene_8032</th>\n      <th>gene_8013</th>\n      <th>gene_16357</th>\n      <th>gene_4209</th>\n      <th>gene_3448</th>\n      <th>gene_6945</th>\n      <th>gene_894</th>\n      <th>gene_7398</th>\n      <th>gene_6355</th>\n      <th>gene_6735</th>\n      <th>gene_3537</th>\n      <th>gene_10363</th>\n      <th>gene_2729</th>\n      <th>gene_9502</th>\n      <th>gene_3962</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18.525161</td>\n      <td>17.173570</td>\n      <td>1.334282</td>\n      <td>0.591871</td>\n      <td>0.591871</td>\n      <td>0.591871</td>\n      <td>6.878308</td>\n      <td>0.000000</td>\n      <td>4.692126</td>\n      <td>0.000000</td>\n      <td>12.205063</td>\n      <td>0.000000</td>\n      <td>13.186662</td>\n      <td>3.266292</td>\n      <td>1.010279</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.478079</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>11.057694</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>10.671541</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.476226</td>\n      <td>4.852678</td>\n      <td>8.522283</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.598651</td>\n      <td>8.086513</td>\n      <td>1.598651</td>\n      <td>0.000000</td>\n      <td>3.410884</td>\n      <td>...</td>\n      <td>0.591871</td>\n      <td>3.410884</td>\n      <td>0.000000</td>\n      <td>3.603763</td>\n      <td>11.244798</td>\n      <td>7.555670</td>\n      <td>1.010279</td>\n      <td>1.010279</td>\n      <td>1.822037</td>\n      <td>11.260690</td>\n      <td>0.000000</td>\n      <td>6.180088</td>\n      <td>1.010279</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.351165</td>\n      <td>12.685279</td>\n      <td>4.063658</td>\n      <td>9.635185</td>\n      <td>0.000000</td>\n      <td>7.555670</td>\n      <td>2.824951</td>\n      <td>6.360101</td>\n      <td>3.017958</td>\n      <td>9.960060</td>\n      <td>1.822037</td>\n      <td>7.896932</td>\n      <td>2.015391</td>\n      <td>5.341922</td>\n      <td>7.455015</td>\n      <td>3.105561</td>\n      <td>5.267892</td>\n      <td>1.334282</td>\n      <td>6.974529</td>\n      <td>2.185898</td>\n      <td>2.824951</td>\n      <td>8.091890</td>\n      <td>8.067547</td>\n      <td>1.822037</td>\n      <td>0.591871</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.609213</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>13.532125</td>\n      <td>0.000000</td>\n      <td>12.695983</td>\n      <td>0.000000</td>\n      <td>10.068832</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.323658</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.555574</td>\n      <td>11.328675</td>\n      <td>0.000000</td>\n      <td>0.323658</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.323658</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.323658</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.975991</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.323658</td>\n      <td>0.587845</td>\n      <td>0.811142</td>\n      <td>...</td>\n      <td>2.006585</td>\n      <td>0.000000</td>\n      <td>5.915519</td>\n      <td>1.004394</td>\n      <td>3.007680</td>\n      <td>11.158837</td>\n      <td>3.293282</td>\n      <td>0.587845</td>\n      <td>0.000000</td>\n      <td>4.903299</td>\n      <td>4.593372</td>\n      <td>8.382698</td>\n      <td>6.204051</td>\n      <td>0.000000</td>\n      <td>1.706508</td>\n      <td>7.623435</td>\n      <td>2.961846</td>\n      <td>0.000000</td>\n      <td>3.499846</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.330264</td>\n      <td>9.702685</td>\n      <td>2.592278</td>\n      <td>7.892907</td>\n      <td>1.004394</td>\n      <td>1.327170</td>\n      <td>0.323658</td>\n      <td>1.327170</td>\n      <td>3.095199</td>\n      <td>2.762370</td>\n      <td>0.323658</td>\n      <td>0.000000</td>\n      <td>9.239379</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.736361</td>\n      <td>0.587845</td>\n      <td>3.890826</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16.053597</td>\n      <td>14.818422</td>\n      <td>1.074163</td>\n      <td>0.452595</td>\n      <td>0.000000</td>\n      <td>1.074163</td>\n      <td>12.900029</td>\n      <td>0.000000</td>\n      <td>14.766151</td>\n      <td>0.000000</td>\n      <td>9.000285</td>\n      <td>0.452595</td>\n      <td>3.000252</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.785739</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.709277</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>1.507160</td>\n      <td>4.009723</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.796598</td>\n      <td>0.000000</td>\n      <td>2.438799</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.306846</td>\n      <td>5.324101</td>\n      <td>0.796598</td>\n      <td>2.228018</td>\n      <td>1.306846</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>1.981122</td>\n      <td>0.796598</td>\n      <td>1.507160</td>\n      <td>5.823816</td>\n      <td>3.065210</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.834509</td>\n      <td>0.796598</td>\n      <td>2.109829</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.074163</td>\n      <td>10.090959</td>\n      <td>12.097624</td>\n      <td>0.000000</td>\n      <td>4.410558</td>\n      <td>0.000000</td>\n      <td>9.410458</td>\n      <td>1.074163</td>\n      <td>1.074163</td>\n      <td>3.718120</td>\n      <td>9.639674</td>\n      <td>4.981168</td>\n      <td>10.172915</td>\n      <td>7.082149</td>\n      <td>2.706508</td>\n      <td>0.796598</td>\n      <td>0.000000</td>\n      <td>1.306846</td>\n      <td>0.000000</td>\n      <td>2.337254</td>\n      <td>0.000000</td>\n      <td>0.452595</td>\n      <td>0.000000</td>\n      <td>8.113753</td>\n      <td>0.452595</td>\n      <td>1.981122</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18.371794</td>\n      <td>17.371079</td>\n      <td>0.434882</td>\n      <td>0.434882</td>\n      <td>1.039419</td>\n      <td>4.216416</td>\n      <td>13.907304</td>\n      <td>0.000000</td>\n      <td>8.653719</td>\n      <td>0.000000</td>\n      <td>9.779831</td>\n      <td>0.000000</td>\n      <td>14.465586</td>\n      <td>0.000000</td>\n      <td>0.768587</td>\n      <td>1.464093</td>\n      <td>0.000000</td>\n      <td>0.768587</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.434882</td>\n      <td>10.421613</td>\n      <td>1.931418</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>7.128840</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.434882</td>\n      <td>1.464093</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.039419</td>\n      <td>6.959747</td>\n      <td>0.434882</td>\n      <td>1.267356</td>\n      <td>0.434882</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>5.942700</td>\n      <td>0.000000</td>\n      <td>5.557180</td>\n      <td>10.321252</td>\n      <td>6.422593</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.039419</td>\n      <td>11.234009</td>\n      <td>0.768587</td>\n      <td>16.445607</td>\n      <td>0.000000</td>\n      <td>1.039419</td>\n      <td>0.000000</td>\n      <td>5.795128</td>\n      <td>12.109086</td>\n      <td>0.000000</td>\n      <td>8.375187</td>\n      <td>0.000000</td>\n      <td>3.439636</td>\n      <td>1.464093</td>\n      <td>3.573556</td>\n      <td>3.656393</td>\n      <td>10.069665</td>\n      <td>1.267356</td>\n      <td>9.323969</td>\n      <td>10.282660</td>\n      <td>1.637239</td>\n      <td>8.867656</td>\n      <td>2.478532</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.246623</td>\n      <td>0.000000</td>\n      <td>0.434882</td>\n      <td>5.029501</td>\n      <td>7.492342</td>\n      <td>0.768587</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>1.580097</td>\n      <td>1.095654</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.360982</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.360982</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.360982</td>\n      <td>0.649386</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.360982</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.975920</td>\n      <td>0.360982</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.095654</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.858777</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.435949</td>\n      <td>0.649386</td>\n      <td>1.435949</td>\n      <td>...</td>\n      <td>1.095654</td>\n      <td>0.889707</td>\n      <td>3.452793</td>\n      <td>1.435949</td>\n      <td>6.571285</td>\n      <td>7.600069</td>\n      <td>1.275841</td>\n      <td>0.649386</td>\n      <td>0.000000</td>\n      <td>4.304657</td>\n      <td>1.711142</td>\n      <td>7.458021</td>\n      <td>2.914239</td>\n      <td>0.000000</td>\n      <td>0.360982</td>\n      <td>11.614167</td>\n      <td>8.338286</td>\n      <td>3.019133</td>\n      <td>4.603502</td>\n      <td>0.000000</td>\n      <td>2.544139</td>\n      <td>1.435949</td>\n      <td>6.974942</td>\n      <td>1.942120</td>\n      <td>8.347130</td>\n      <td>8.453908</td>\n      <td>4.196969</td>\n      <td>0.360982</td>\n      <td>8.194501</td>\n      <td>2.231279</td>\n      <td>7.361567</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.205037</td>\n      <td>0.360982</td>\n      <td>0.000000</td>\n      <td>4.304657</td>\n      <td>0.360982</td>\n      <td>0.889707</td>\n      <td>0.889707</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>17.398084</td>\n      <td>16.064835</td>\n      <td>0.894294</td>\n      <td>0.515410</td>\n      <td>0.000000</td>\n      <td>2.404276</td>\n      <td>13.971283</td>\n      <td>0.000000</td>\n      <td>7.604701</td>\n      <td>0.000000</td>\n      <td>10.640724</td>\n      <td>0.000000</td>\n      <td>12.293564</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.148934</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10.685423</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>13.316805</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.194150</td>\n      <td>0.000000</td>\n      <td>2.621149</td>\n      <td>7.968454</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.838427</td>\n      <td>2.895400</td>\n      <td>2.404276</td>\n      <td>0.000000</td>\n      <td>3.881645</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>9.007097</td>\n      <td>0.000000</td>\n      <td>2.282232</td>\n      <td>11.564378</td>\n      <td>3.052990</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.894294</td>\n      <td>6.798932</td>\n      <td>2.002018</td>\n      <td>6.012269</td>\n      <td>0.515410</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.258687</td>\n      <td>13.442477</td>\n      <td>2.976345</td>\n      <td>4.809939</td>\n      <td>0.000000</td>\n      <td>6.966800</td>\n      <td>3.195033</td>\n      <td>6.646511</td>\n      <td>3.552648</td>\n      <td>9.409034</td>\n      <td>2.282232</td>\n      <td>9.645504</td>\n      <td>4.853447</td>\n      <td>5.178245</td>\n      <td>8.830585</td>\n      <td>5.817332</td>\n      <td>1.653885</td>\n      <td>3.052990</td>\n      <td>6.563805</td>\n      <td>0.000000</td>\n      <td>0.515410</td>\n      <td>6.337401</td>\n      <td>7.436104</td>\n      <td>0.894294</td>\n      <td>1.838427</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.683107</td>\n      <td>6.880294</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.191152</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.042924</td>\n      <td>0.000000</td>\n      <td>10.817295</td>\n      <td>3.602386</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.756327</td>\n      <td>0.919683</td>\n      <td>0.531868</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.531868</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.078033</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.742188</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.919683</td>\n      <td>6.570924</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>9.132911</td>\n      <td>5.009356</td>\n      <td>4.200646</td>\n      <td>...</td>\n      <td>1.476796</td>\n      <td>0.531868</td>\n      <td>0.919683</td>\n      <td>3.800030</td>\n      <td>6.923316</td>\n      <td>2.561717</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.531868</td>\n      <td>5.779932</td>\n      <td>0.919683</td>\n      <td>6.140568</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.919683</td>\n      <td>2.191152</td>\n      <td>4.128937</td>\n      <td>0.000000</td>\n      <td>4.301946</td>\n      <td>0.531868</td>\n      <td>0.531868</td>\n      <td>0.531868</td>\n      <td>6.891176</td>\n      <td>5.791590</td>\n      <td>3.492379</td>\n      <td>8.419990</td>\n      <td>7.193190</td>\n      <td>0.919683</td>\n      <td>0.531868</td>\n      <td>3.309846</td>\n      <td>6.131423</td>\n      <td>2.856308</td>\n      <td>0.000000</td>\n      <td>4.673596</td>\n      <td>0.000000</td>\n      <td>0.531868</td>\n      <td>0.919683</td>\n      <td>5.683107</td>\n      <td>2.561717</td>\n      <td>2.191152</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>17.604560</td>\n      <td>15.748748</td>\n      <td>0.000000</td>\n      <td>0.441802</td>\n      <td>1.655260</td>\n      <td>0.441802</td>\n      <td>12.303729</td>\n      <td>2.500241</td>\n      <td>6.936190</td>\n      <td>0.000000</td>\n      <td>9.570964</td>\n      <td>0.441802</td>\n      <td>13.032378</td>\n      <td>2.196261</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.196261</td>\n      <td>0.000000</td>\n      <td>2.484628</td>\n      <td>0.000000</td>\n      <td>11.688014</td>\n      <td>0.441802</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>5.604463</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.297833</td>\n      <td>11.066547</td>\n      <td>0.441802</td>\n      <td>0.441802</td>\n      <td>8.208849</td>\n      <td>5.943729</td>\n      <td>2.078849</td>\n      <td>0.000000</td>\n      <td>2.588829</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>5.784046</td>\n      <td>0.000000</td>\n      <td>0.779554</td>\n      <td>9.601893</td>\n      <td>2.672290</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>10.887396</td>\n      <td>1.481041</td>\n      <td>5.726632</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.656628</td>\n      <td>8.103786</td>\n      <td>1.282855</td>\n      <td>9.490610</td>\n      <td>0.000000</td>\n      <td>5.023694</td>\n      <td>2.405856</td>\n      <td>6.775617</td>\n      <td>2.500241</td>\n      <td>13.904296</td>\n      <td>5.811918</td>\n      <td>8.126162</td>\n      <td>2.078849</td>\n      <td>6.117577</td>\n      <td>6.259274</td>\n      <td>4.036213</td>\n      <td>0.441802</td>\n      <td>0.000000</td>\n      <td>7.404273</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>8.403067</td>\n      <td>5.310914</td>\n      <td>0.000000</td>\n      <td>2.500241</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>1.683921</td>\n      <td>0.352307</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.352307</td>\n      <td>1.554049</td>\n      <td>0.635336</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.074848</td>\n      <td>1.411318</td>\n      <td>0.000000</td>\n      <td>0.635336</td>\n      <td>0.635336</td>\n      <td>0.352307</td>\n      <td>0.635336</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.635336</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.352307</td>\n      <td>0.352307</td>\n      <td>0.352307</td>\n      <td>0.635336</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.352307</td>\n      <td>1.913148</td>\n      <td>3.173463</td>\n      <td>0.000000</td>\n      <td>0.352307</td>\n      <td>...</td>\n      <td>3.835722</td>\n      <td>0.000000</td>\n      <td>4.663515</td>\n      <td>3.217045</td>\n      <td>1.074848</td>\n      <td>10.243912</td>\n      <td>3.943406</td>\n      <td>1.554049</td>\n      <td>0.000000</td>\n      <td>6.092121</td>\n      <td>1.803062</td>\n      <td>6.671973</td>\n      <td>8.498662</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.281609</td>\n      <td>4.019204</td>\n      <td>3.943406</td>\n      <td>2.015391</td>\n      <td>0.000000</td>\n      <td>8.200766</td>\n      <td>1.411318</td>\n      <td>3.173463</td>\n      <td>3.778713</td>\n      <td>4.615481</td>\n      <td>6.996547</td>\n      <td>4.386604</td>\n      <td>2.015391</td>\n      <td>11.455713</td>\n      <td>0.635336</td>\n      <td>3.217045</td>\n      <td>0.635336</td>\n      <td>0.000000</td>\n      <td>7.416570</td>\n      <td>0.000000</td>\n      <td>0.635336</td>\n      <td>3.379247</td>\n      <td>7.754473</td>\n      <td>2.015391</td>\n      <td>0.635336</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>17.853785</td>\n      <td>16.780578</td>\n      <td>0.724214</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.635987</td>\n      <td>14.208814</td>\n      <td>0.724214</td>\n      <td>15.988685</td>\n      <td>0.000000</td>\n      <td>10.568764</td>\n      <td>0.000000</td>\n      <td>3.742858</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.204141</td>\n      <td>0.000000</td>\n      <td>1.204141</td>\n      <td>0.000000</td>\n      <td>10.994205</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>7.742835</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.326243</td>\n      <td>3.939593</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>6.197150</td>\n      <td>1.204141</td>\n      <td>0.000000</td>\n      <td>1.204141</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>2.910733</td>\n      <td>0.000000</td>\n      <td>8.071307</td>\n      <td>9.113463</td>\n      <td>6.005483</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>5.438951</td>\n      <td>7.819790</td>\n      <td>0.000000</td>\n      <td>4.613054</td>\n      <td>0.724214</td>\n      <td>0.724214</td>\n      <td>2.090853</td>\n      <td>5.326243</td>\n      <td>10.257565</td>\n      <td>2.090853</td>\n      <td>5.749848</td>\n      <td>0.000000</td>\n      <td>2.296311</td>\n      <td>1.563646</td>\n      <td>4.574095</td>\n      <td>2.779890</td>\n      <td>11.163694</td>\n      <td>3.939593</td>\n      <td>9.350933</td>\n      <td>2.910733</td>\n      <td>4.534055</td>\n      <td>8.632130</td>\n      <td>3.999630</td>\n      <td>0.000000</td>\n      <td>2.090853</td>\n      <td>3.595014</td>\n      <td>0.000000</td>\n      <td>2.779890</td>\n      <td>8.245676</td>\n      <td>6.118104</td>\n      <td>0.000000</td>\n      <td>2.476122</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 1000 columns</p>\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## Reshape and split the data"],"metadata":{}},{"cell_type":"code","source":["# change to pandas dataframe\ndata = r_data_ext.toPandas()\nindexed_label = indexed_r_label.toPandas()\n# transfer to numpy\ndata_np = np.array(data)\nindexed_label_np = np.array(indexed_label)\n\n# reshape to fit the LSTM expectation\ndata_np = data_np.reshape(data_np.shape[0],1,data_np.shape[1])\nindexed_label_np = indexed_label_np.reshape(indexed_label_np.shape[0],1,indexed_label_np.shape[1])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["# split the data to be 70/30\nX_train, X_test, y_train, y_test = train_test_split(data_np, indexed_label_np, test_size=0.3, random_state = 123)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["*Traning Data and Test Data Overview*"],"metadata":{}},{"cell_type":"code","source":["# tranfer back to label\ndef pred_trans(x):\n  pred_ = ['0'] * len(x)\n  for row in range(len(x)):\n    if x[row][0] == x[row].max():\n       pred_[row] = 'BRCA'\n    elif x[row][1] == x[row].max():\n       pred_[row] = 'KIRC'\n    elif x[row][2] == x[row].max():\n       pred_[row] = 'LUAD'\n    elif x[row][3] == x[row].max():\n       pred_[row] = 'PRAD'\n    else:\n       pred_[row] = 'COAD'\n  return pred_"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["# converse to 2D for viz\ny_train = y_train.reshape(-1,5)\ny_test = y_test.reshape(-1,5)\n\n# transfer the training dataset to label\ny_train_label = pred_trans(y_train)\n\n# transfer the test dataset to label\ny_test_label = pred_trans(y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["labels = ['BRCA','KIRC','LUAD','PRAD','COAD']\n\n# Create subplots\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values = \\\n              [y_train_label.count('BRCA'),y_train_label.count('KIRC'),y_train_label.count('LUAD'),y_train_label.count('PRAD'),y_train_label.count('COAD')], name=\"Training Label\"), \\\n              1, 1)\nfig.add_trace(go.Pie(labels=labels, values = \\\n              [y_test_label.count('BRCA'),y_test_label.count('KIRC'),y_test_label.count('LUAD'),y_test_label.count('PRAD'),y_test_label.count('COAD')], name=\"Test Label\"), \\\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.6, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Traning Label and Test Label Distribution\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Traning Label', x=0.18, y=0.5, font_size=15, showarrow=False),\n                 dict(text='Test Label', x=0.82, y=0.5, font_size=15, showarrow=False)])\nfig.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>\n            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n            <div id=\"0fa8c2ea-f65f-4808-8b56-c901528b7edb\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>\n            <script type=\"text/javascript\">\n                \n                    window.PLOTLYENV=window.PLOTLYENV || {};\n                    \n                if (document.getElementById(\"0fa8c2ea-f65f-4808-8b56-c901528b7edb\")) {\n                    Plotly.newPlot(\n                        '0fa8c2ea-f65f-4808-8b56-c901528b7edb',\n                        [{\"domain\": {\"x\": [0.0, 0.45], \"y\": [0.0, 1.0]}, \"hole\": 0.6, \"hoverinfo\": \"label+percent+name\", \"labels\": [\"BRCA\", \"KIRC\", \"LUAD\", \"PRAD\", \"COAD\"], \"name\": \"Training Label\", \"type\": \"pie\", \"values\": [216, 94, 106, 95, 49]}, {\"domain\": {\"x\": [0.55, 1.0], \"y\": [0.0, 1.0]}, \"hole\": 0.6, \"hoverinfo\": \"label+percent+name\", \"labels\": [\"BRCA\", \"KIRC\", \"LUAD\", \"PRAD\", \"COAD\"], \"name\": \"Test Label\", \"type\": \"pie\", \"values\": [84, 52, 35, 41, 29]}],\n                        {\"annotations\": [{\"font\": {\"size\": 15}, \"showarrow\": false, \"text\": \"Traning Label\", \"x\": 0.18, \"y\": 0.5}, {\"font\": {\"size\": 15}, \"showarrow\": false, \"text\": \"Test Label\", \"x\": 0.82, \"y\": 0.5}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Traning Label and Test Label Distribution\"}},\n                        {\"responsive\": true}\n                    )\n                };\n                \n            </script>\n        </div>\n</body>\n</html>"]}}],"execution_count":30},{"cell_type":"markdown","source":["The training data and test data share a similar distribution, which is good for our further validation and testing process to diminish the bias underlying samples."],"metadata":{}},{"cell_type":"markdown","source":["## Deep Learning Architecture"],"metadata":{}},{"cell_type":"markdown","source":["<img src=\"https://github.com/StacyYin/Data-Visualization-/raw/master/PanCan.jpg\" alt=\"Image\" border=\"0\">"],"metadata":{}},{"cell_type":"markdown","source":["### Set the Hyperparameters"],"metadata":{}},{"cell_type":"markdown","source":["Hyperparameters were initiated based on the sample's size and target values."],"metadata":{}},{"cell_type":"code","source":["# Hyperparameter Initialization\nNum_Class = 5\nNum_Inputs = X_train.shape[2]\nNum_Hidden = 500\n\nprint('Num_Class: ', Num_Class)\nprint('\\nNum_Inputs: ', Num_Inputs)\nprint('\\nNum_Hidden: ', Num_Hidden)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Num_Class:  5\n\nNum_Inputs:  1000\n\nNum_Hidden:  500\n</div>"]}}],"execution_count":36},{"cell_type":"markdown","source":["### Model Construction"],"metadata":{}},{"cell_type":"markdown","source":["Basic models can be optimized through different hyperparameters. Here learning rate and adam decay were chosen for the model tuning."],"metadata":{}},{"cell_type":"markdown","source":["## Learning Rate / Adam Decay Tuning Based on Gaussian Process"],"metadata":{}},{"cell_type":"markdown","source":["Learning Rate can be important factors which decide how soon the model will adapt to the problem. A large learning rate may miss the optimal point for modeling. So here a range of values starting from 1e-5 to 1e-3 were set.\nAdam Decay was also experimented with this range."],"metadata":{}},{"cell_type":"code","source":["# set the range of learning rate and adam decay\ndim_learning_rate = Real(low=1e-5, high=1e-3, prior='log-uniform',\n                         name='learning_rate')\ndim_adam_decay = Real(low=1e-5,high=1e-3,name=\"adam_decay\")\n\n# tuning part\ndimensions = [dim_learning_rate, dim_adam_decay]\n# set the default value for start\ndefault_parameters = [1e-3, 1e-3]\n\n# create the model based on Sequential layers\ndef create_model(learning_rate, adam_decay):\n    model = Sequential()\n    \n    model.add(LSTM(Num_Hidden, input_shape = (1, Num_Inputs), activity_regularizer = regularizers.l2(1e-5), return_sequences = True))\n    model.add(Activation('relu'))\n    model.add(Dropout(rate=0.1))\n\n    model.add(Dense(Num_Class))\n    model.add(Activation('softmax')) \n\n    # setup our optimizer and compile\n    adam = Adam(lr = learning_rate, decay = adam_decay)\n    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics=['accuracy'])\n    \n    return model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["@use_named_args(dimensions=dimensions)\ndef fitness(learning_rate, adam_decay):\n  \n    model = create_model(learning_rate = learning_rate,\n                         adam_decay = adam_decay)\n    \n    \n    # fit the model                  \n    b_box = model.fit(x=X_train,\n                      y=y_train,\n                      epochs = 5,\n                      batch_size = 128,\n                      validation_split=0.1,\n                      )\n    # return the validation loss for the last epoch\n    loss = b_box.history['val_loss'][-1]\n\n    # delete the Keras model with these hyper-parameters from memory\n    del model\n    \n    # clear the Keras session, otherwise it will keep adding new\n    # models to the same TensorFlow graph each time we create\n    # a model with a different set of hyper-parameters\n    K.clear_session()\n    tensorflow.reset_default_graph()\n    \n    # the optimizer aims for the lowest score\n    return loss"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"markdown","source":["#### Bayesian Optimization"],"metadata":{}},{"cell_type":"markdown","source":["Bayesian optimization is an approach to optimizing objective functions. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample."],"metadata":{}},{"cell_type":"code","source":["# Parameters Searching\ngp_result = gp_minimize(func = fitness,\n                        dimensions = dimensions,\n                        n_calls = 15, #minimum value is 12\n                        noise = 0.0001,\n                        n_jobs = -1,\n                        kappa = 5,\n                        x0 = default_parameters)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Train on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0711\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0715\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 4s 7ms/step - loss: 0.0705 - val_loss: 0.0520\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0698 - val_loss: 0.0515\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0709\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0700\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0694 - val_loss: 0.0510\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0687\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0689\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0684 - val_loss: 0.0505\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0678\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0678\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0677 - val_loss: 0.0501\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0683\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0678\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0680\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 6ms/step - loss: 0.0669 - val_loss: 0.0496\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0673\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0673\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0664 - val_loss: 0.0491\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0671\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0666\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0654 - val_loss: 0.0487\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0659\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0657\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0656\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0650 - val_loss: 0.0485\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0654\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0654\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0646 - val_loss: 0.0482\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0648\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0647\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 6ms/step - loss: 0.0639 - val_loss: 0.0479\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0649\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0635 - val_loss: 0.0477\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0646\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0646\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0631 - val_loss: 0.0473\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0640\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0626 - val_loss: 0.0470\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0627\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0622\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0628\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0619 - val_loss: 0.0467\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0619\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0613 - val_loss: 0.0463\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0618\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0610 - val_loss: 0.0460\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0609\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0611\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0612\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0606 - val_loss: 0.0458\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0612\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0605 - val_loss: 0.0455\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0598\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0609\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0601 - val_loss: 0.0452\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0607\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0608\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 6ms/step - loss: 0.0594 - val_loss: 0.0450\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0604\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0600\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0592 - val_loss: 0.0449\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0599\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0590 - val_loss: 0.0447\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0590\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0590\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0585 - val_loss: 0.0445\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0601\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0595\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0591\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0583 - val_loss: 0.0443\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0596\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0598\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0589\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 6ms/step - loss: 0.0581 - val_loss: 0.0441\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0581\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0580\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0577 - val_loss: 0.0439\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0568\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0572\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0578\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0572 - val_loss: 0.0437\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0572\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0573\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0579\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0573 - val_loss: 0.0435\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0571\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0574\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0577\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0569 - val_loss: 0.0433\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0574\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0579\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0581\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0571 - val_loss: 0.0429\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0569\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0562 - val_loss: 0.0426\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0550\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0565\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0559 - val_loss: 0.0424\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0567\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0563\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0555 - val_loss: 0.0422\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0568\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0563\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0559\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0554 - val_loss: 0.0421\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0556\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0560\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0556\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0551 - val_loss: 0.0419\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0561\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0559\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0555\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0550 - val_loss: 0.0417\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0544 - val_loss: 0.0416\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0556\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0554\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0545 - val_loss: 0.0415\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0546\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0543 - val_loss: 0.0413\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0549\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0538 - val_loss: 0.0411\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0538 - val_loss: 0.0410\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0543\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0542\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0534 - val_loss: 0.0408\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0541\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0538\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0531 - val_loss: 0.0407\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0534\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0528 - val_loss: 0.0405\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0527\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0525 - val_loss: 0.0403\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0532\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0526 - val_loss: 0.0400\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0528\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0528\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0529\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0523 - val_loss: 0.0398\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0522\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0522\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0528\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0521 - val_loss: 0.0397\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0527\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0517 - val_loss: 0.0396\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 6ms/step - loss: 0.0517 - val_loss: 0.0393\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0517\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0513 - val_loss: 0.0392\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0520\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0513 - val_loss: 0.0391\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0517\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0519\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0511 - val_loss: 0.0390\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0528\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0526\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0518\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0510 - val_loss: 0.0389\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0512\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0512\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0507 - val_loss: 0.0389\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0517\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0507 - val_loss: 0.0387\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0507\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0512\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0510\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0504 - val_loss: 0.0385\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0509\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0507\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0508\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0499 - val_loss: 0.0383\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0505\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0502\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0501\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0498 - val_loss: 0.0383\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0506\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0497\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 6ms/step - loss: 0.0496 - val_loss: 0.0381\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0502\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0501\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0495 - val_loss: 0.0380\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0493\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0503\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0496\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0491 - val_loss: 0.0380\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0493\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0487 - val_loss: 0.0380\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0490\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0495\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0492\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0488 - val_loss: 0.0379\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0485 - val_loss: 0.0377\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0482\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0490\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0484 - val_loss: 0.0375\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0489\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0490\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0485 - val_loss: 0.0373\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0493\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0492\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0484 - val_loss: 0.0372\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0490\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0481 - val_loss: 0.0372\n/databricks/python/lib/python3.7/site-packages/skopt/optimizer/optimizer.py:409: UserWarning:\n\nThe objective has been evaluated at this point before.\n\nTrain on 504 samples, validate on 56 samples\nEpoch 1/5\n\r128/504 [======&gt;.......................] - ETA: 7s - loss: 0.0484\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 2s - loss: 0.0484\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0483\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 3s 7ms/step - loss: 0.0478 - val_loss: 0.0371\nEpoch 2/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0500\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0479 - val_loss: 0.0368\nEpoch 3/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0473\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0472 - val_loss: 0.0367\nEpoch 4/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0480\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0482\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0475 - val_loss: 0.0367\nEpoch 5/5\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.0477\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.0478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.0478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.0472 - val_loss: 0.0367\n</div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["#### Best Model for Training"],"metadata":{}},{"cell_type":"markdown","source":["*Optimal Hyperparameters*"],"metadata":{}},{"cell_type":"code","source":["# fetch the optimal parameters\noptimal_learning_rate = gp_result.x[0] # the optimal gp_result.x[0]  is 0.0001\noptimal_Adam_decay =gp_result.x[1] # the optimal gp_result.x[1] is 0.0001\n\nprint(optimal_learning_rate)\nprint(optimal_Adam_decay)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.0001\n0.0001\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["# create the model framework\nmodel = create_model(optimal_learning_rate,optimal_Adam_decay)\n\n# get the model overview\nmodel.summary()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n\nModel: &#34;sequential_1&#34;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_1 (LSTM)                (None, 1, 500)            3002000   \n_________________________________________________________________\nactivation_1 (Activation)    (None, 1, 500)            0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1, 500)            0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 1, 5)              2505      \n_________________________________________________________________\nactivation_2 (Activation)    (None, 1, 5)              0         \n=================================================================\nTotal params: 3,004,505\nTrainable params: 3,004,505\nNon-trainable params: 0\n_________________________________________________________________\n</div>"]}}],"execution_count":49},{"cell_type":"markdown","source":["## Train the Neural Network with Early Stopping"],"metadata":{}},{"cell_type":"code","source":["# show the training result - running 10 epochs with early stopping\nhistory = model.fit(X_train,y_train, epochs = 10, batch_size = 128, validation_split = 0.1,callbacks =[EarlyStopping(monitor='val_acc', mode='max')])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nTrain on 504 samples, validate on 56 samples\nEpoch 1/10\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\n\r128/504 [======&gt;.......................] - ETA: 8s - loss: 1.6886 - acc: 0.1562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 3s - loss: 1.5900 - acc: 0.2695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 1s - loss: 1.4945 - acc: 0.4271\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 4s 8ms/step - loss: 1.4147 - acc: 0.5377 - val_loss: 0.9886 - val_acc: 0.9286\nEpoch 2/10\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 1.0031 - acc: 0.9531\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.8995 - acc: 0.9766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.8506 - acc: 0.9792\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.7931 - acc: 0.9821 - val_loss: 0.5408 - val_acc: 0.9821\nEpoch 3/10\n\r128/504 [======&gt;.......................] - ETA: 0s - loss: 0.5419 - acc: 1.0000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r256/504 [==============&gt;...............] - ETA: 0s - loss: 0.5072 - acc: 0.9961\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r384/504 [=====================&gt;........] - ETA: 0s - loss: 0.4686 - acc: 0.9974\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r504/504 [==============================] - 1s 1ms/step - loss: 0.4465 - acc: 0.9960 - val_loss: 0.3118 - val_acc: 0.9821\n</div>"]}}],"execution_count":51},{"cell_type":"markdown","source":["## Model Evaluation and Prediction"],"metadata":{}},{"cell_type":"markdown","source":["*Model Evaluation*\n\nThe whole training data was left 10% for validation, which can be used to evaluate the model performance."],"metadata":{}},{"cell_type":"markdown","source":["*Validation Loss*\n\nLoss is defined as categorical cross entropy loss, which is a metric for evaluating a good classification."],"metadata":{}},{"cell_type":"code","source":["# display the validation loss as the epochs rise\nfig1 = go.Figure()\nfig1.add_trace(go.Scatter(y=history.history['val_loss'],\n                    mode='lines',\n                    name='training_loss'))\nannotations = []\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=2,\n                              xanchor='left', yanchor='bottom',\n                              text='Validation Loss as Epochs Rise',\n                              font=dict(family='Arial',\n                                        size=20,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig1.update_layout(annotations = annotations)\nfig1.update_xaxes(title_text='Epoch')\nfig1.update_yaxes(title_text='Loss')\nfig1.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>\n            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n            <div id=\"ec44561f-f873-4773-9238-8fce496c5c9c\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>\n            <script type=\"text/javascript\">\n                \n                    window.PLOTLYENV=window.PLOTLYENV || {};\n                    \n                if (document.getElementById(\"ec44561f-f873-4773-9238-8fce496c5c9c\")) {\n                    Plotly.newPlot(\n                        'ec44561f-f873-4773-9238-8fce496c5c9c',\n                        [{\"mode\": \"lines\", \"name\": \"training_loss\", \"type\": \"scatter\", \"y\": [0.9885606169700623, 0.5407763123512268, 0.31175464391708374]}],\n                        {\"annotations\": [{\"font\": {\"color\": \"rgb(37,37,37)\", \"family\": \"Arial\", \"size\": 20}, \"showarrow\": false, \"text\": \"Validation Loss as Epochs Rise\", \"x\": 0.0, \"xanchor\": \"left\", \"xref\": \"paper\", \"y\": 2, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"title\": {\"text\": \"Loss\"}}},\n                        {\"responsive\": true}\n                    )\n                };\n                \n            </script>\n        </div>\n</body>\n</html>"]}}],"execution_count":55},{"cell_type":"markdown","source":["As epochs rise, the validation loss drops significantly."],"metadata":{}},{"cell_type":"markdown","source":["*Validation Accuracy* \n\nAccuracy is calculated by number of correct predictions divided by total number of predictions."],"metadata":{}},{"cell_type":"code","source":["# display the validation accuracy as the epochs rise\nfig2 = go.Figure()\nfig2.add_trace(go.Scatter(y=history.history['val_acc'],\n                    mode='lines+markers',\n                    name='training_accuracy'))\nannotations = []\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,\n                              xanchor='left', yanchor='bottom',\n                              text='Validation Accuracy as Epochs Rise',\n                              font=dict(family='Arial',\n                                        size=20,\n                                        color='rgb(37,37,37)'),\n                              showarrow=False))\nfig2.update_layout(annotations = annotations)\nfig2.update_xaxes(title_text='Epoch')\nfig2.update_yaxes(title_text='Accuracy')\nfig2.update_yaxes(range=[0.8, 1])\nfig2.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>\n            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n            <div id=\"74459776-289f-49ca-85fa-33a9d55e785b\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>\n            <script type=\"text/javascript\">\n                \n                    window.PLOTLYENV=window.PLOTLYENV || {};\n                    \n                if (document.getElementById(\"74459776-289f-49ca-85fa-33a9d55e785b\")) {\n                    Plotly.newPlot(\n                        '74459776-289f-49ca-85fa-33a9d55e785b',\n                        [{\"mode\": \"lines+markers\", \"name\": \"training_accuracy\", \"type\": \"scatter\", \"y\": [0.9285714030265808, 0.9821428656578064, 0.9821428656578064]}],\n                        {\"annotations\": [{\"font\": {\"color\": \"rgb(37,37,37)\", \"family\": \"Arial\", \"size\": 20}, \"showarrow\": false, \"text\": \"Validation Accuracy as Epochs Rise\", \"x\": 0.0, \"xanchor\": \"left\", \"xref\": \"paper\", \"y\": 1.05, \"yanchor\": \"bottom\", \"yref\": \"paper\"}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"title\": {\"text\": \"Epoch\"}}, \"yaxis\": {\"range\": [0.8, 1], \"title\": {\"text\": \"Accuracy\"}}},\n                        {\"responsive\": true}\n                    )\n                };\n                \n            </script>\n        </div>\n</body>\n</html>"]}}],"execution_count":58},{"cell_type":"markdown","source":["Validation Accuracy achieved a remarkable high level after learning."],"metadata":{}},{"cell_type":"markdown","source":["*Model Prediction*"],"metadata":{}},{"cell_type":"code","source":["# predict the test result\ny_test_pred = model.predict(X_test)\n# round up to 4 decimals\ny_test_pred_r2 = np.around(y_test_pred, decimals = 4)\n# transfer back to 2 dimensions\ny_test_pred_r2 = y_test_pred_r2.reshape(-1,5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":61},{"cell_type":"code","source":["# show the test result\ny_test_pred_pd = pd.DataFrame(y_test_pred_r2, columns=[\"BRCA\", \"KIRC\", \"LUAD\",\"PRAD\",\"COAD\"])\ny_test_pred_pd.index.name = 'PID'\ny_test_pred_pd.head(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BRCA</th>\n      <th>KIRC</th>\n      <th>LUAD</th>\n      <th>PRAD</th>\n      <th>COAD</th>\n    </tr>\n    <tr>\n      <th>PID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0687</td>\n      <td>0.0657</td>\n      <td>0.1477</td>\n      <td>0.0380</td>\n      <td>0.6799</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0979</td>\n      <td>0.0346</td>\n      <td>0.7377</td>\n      <td>0.0708</td>\n      <td>0.0590</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0665</td>\n      <td>0.0132</td>\n      <td>0.8636</td>\n      <td>0.0291</td>\n      <td>0.0275</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0406</td>\n      <td>0.8731</td>\n      <td>0.0400</td>\n      <td>0.0167</td>\n      <td>0.0295</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.1818</td>\n      <td>0.0315</td>\n      <td>0.1107</td>\n      <td>0.6506</td>\n      <td>0.0254</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":62},{"cell_type":"markdown","source":["The model would automatically select the cancer type with the highest probability to be the prediction result for each patient."],"metadata":{}},{"cell_type":"markdown","source":["## Test Accuracy"],"metadata":{}},{"cell_type":"code","source":["# print out the test loss and accuracy\ny_test = y_test.reshape(y_test.shape[0],1,y_test.shape[1])\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('Test loss:', test_loss)\nprint('Test accuracy:', test_acc)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r 32/241 [==&gt;...........................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r160/241 [==================&gt;...........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r241/241 [==============================] - 0s 516us/step\nTest loss: 0.29311567396543825\nTest accuracy: 1.0\n</div>"]}}],"execution_count":65},{"cell_type":"markdown","source":["## Test Original Label & Test Prediction Label"],"metadata":{}},{"cell_type":"code","source":["# transfer the training dataset to label\nTest_Prediction = pred_trans(y_test_pred_r2)\n\n# transfer the test dataset to label\nTest_Original = pred_trans(y_test)\n\nlabels = ['BRCA','KIRC','LUAD','PRAD','COAD']\n\n# Create subplots\nfig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\nfig.add_trace(go.Pie(labels=labels, values = \\\n              [Test_Original.count('BRCA'),Test_Original.count('KIRC'), \\\n               Test_Original.count('LUAD'),Test_Original.count('PRAD'),Test_Original.count('COAD')], name=\"Test Original\"), \\\n              1, 1)\nfig.add_trace(go.Pie(labels=labels, values = \\\n       [Test_Prediction.count('BRCA'),Test_Prediction.count('KIRC'),\\\n        Test_Prediction.count('LUAD'),Test_Prediction.count('PRAD'),Test_Prediction.count('COAD')], name=\"Test Prediction\"), \\\n              1, 2)\n\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.6, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Test Original and Test Prediction Distribution\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Test Original', x=0.18, y=0.5, font_size=15, showarrow=False),\n                 dict(text='Test Prediction', x=0.82, y=0.5, font_size=15, showarrow=False)])\nfig.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<html>\n<head><meta charset=\"utf-8\" /></head>\n<body>\n    <div>\n            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n            <div id=\"3052c40d-1ada-4a83-9371-bf4236f64b72\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>\n            <script type=\"text/javascript\">\n                \n                    window.PLOTLYENV=window.PLOTLYENV || {};\n                    \n                if (document.getElementById(\"3052c40d-1ada-4a83-9371-bf4236f64b72\")) {\n                    Plotly.newPlot(\n                        '3052c40d-1ada-4a83-9371-bf4236f64b72',\n                        [{\"domain\": {\"x\": [0.0, 0.45], \"y\": [0.0, 1.0]}, \"hole\": 0.6, \"hoverinfo\": \"label+percent+name\", \"labels\": [\"BRCA\", \"KIRC\", \"LUAD\", \"PRAD\", \"COAD\"], \"name\": \"Test Original\", \"type\": \"pie\", \"values\": [84, 52, 35, 41, 29]}, {\"domain\": {\"x\": [0.55, 1.0], \"y\": [0.0, 1.0]}, \"hole\": 0.6, \"hoverinfo\": \"label+percent+name\", \"labels\": [\"BRCA\", \"KIRC\", \"LUAD\", \"PRAD\", \"COAD\"], \"name\": \"Test Prediction\", \"type\": \"pie\", \"values\": [84, 52, 35, 41, 29]}],\n                        {\"annotations\": [{\"font\": {\"size\": 15}, \"showarrow\": false, \"text\": \"Test Original\", \"x\": 0.18, \"y\": 0.5}, {\"font\": {\"size\": 15}, \"showarrow\": false, \"text\": \"Test Prediction\", \"x\": 0.82, \"y\": 0.5}], \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Test Original and Test Prediction Distribution\"}},\n                        {\"responsive\": true}\n                    )\n                };\n                \n            </script>\n        </div>\n</body>\n</html>"]}}],"execution_count":67},{"cell_type":"markdown","source":["The prediction amazingly covered the test original label, which approved a highly efficient method to identifying the tumor types."],"metadata":{}},{"cell_type":"markdown","source":["## Save to Gold Bucket in Delta"],"metadata":{}},{"cell_type":"markdown","source":["A Delta table can easily be modified through inserts, deletes, and merges. In addition, all these modifications can be rolled back to obtain an older version of the Delta Table. That way Delta Lake offers us **flexible storage** and helps us to keep control over the changes in the data."],"metadata":{}},{"cell_type":"code","source":["# transfer back to Spark Dataframe\ny_test_pred_sp = spark.createDataFrame(y_test_pred_pd)\n\n# add the PID column\ny_test_pred_sp2 = y_test_pred_sp.repartition(1).withColumn(\"PID\",monotonically_increasing_id())\ny_test_pred_sp2 = y_test_pred_sp2.select(\"PID\",\"BRCA\",\"KIRC\",\"LUAD\",\"PRAD\",\"COAD\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":71},{"cell_type":"code","source":["write_file_name = 'PanCan_TestPred'\nwrite_location = f'{MOUNT_GOLD}/{write_file_name}'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":72},{"cell_type":"code","source":["# write to the gold bucket\ny_test_pred_sp2.write.format(\"delta\") \\\n               .option(\"overwriteSchema\", \"true\") \\\n               .mode(\"overwrite\") \\\n               .save(write_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":73},{"cell_type":"markdown","source":["*Load From Gold Bucket*"],"metadata":{}},{"cell_type":"markdown","source":["## Business Value - Running SQL Queries against Delta"],"metadata":{}},{"cell_type":"code","source":["db_name = 'PanCan'\nspark.sql(f\"\"\"\n   CREATE DATABASE IF NOT EXISTS {db_name}\n   \"\"\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[69]: DataFrame[]</div>"]}}],"execution_count":76},{"cell_type":"code","source":["spark.sql(f\"\"\"\n    CREATE TABLE IF NOT EXISTS {db_name}.{write_file_name}\n    USING DELTA\n    LOCATION '{write_location}'\n\"\"\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[70]: DataFrame[]</div>"]}}],"execution_count":77},{"cell_type":"code","source":["test_res = spark.sql(f\"\"\"\n    SELECT * FROM {db_name}.{write_file_name}\n    LIMIT 5\n\"\"\")\ndisplay(test_res)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PID</th><th>BRCA</th><th>KIRC</th><th>LUAD</th><th>PRAD</th><th>COAD</th></tr></thead><tbody><tr><td>0</td><td>0.0687</td><td>0.0657</td><td>0.1477</td><td>0.038</td><td>0.6799</td></tr><tr><td>1</td><td>0.0979</td><td>0.0346</td><td>0.7377</td><td>0.0708</td><td>0.059</td></tr><tr><td>2</td><td>0.0665</td><td>0.0132</td><td>0.8636</td><td>0.0291</td><td>0.0275</td></tr><tr><td>3</td><td>0.0406</td><td>0.8731</td><td>0.04</td><td>0.0167</td><td>0.0295</td></tr><tr><td>4</td><td>0.1818</td><td>0.0315</td><td>0.1107</td><td>0.6506</td><td>0.0254</td></tr></tbody></table></div>"]}}],"execution_count":78},{"cell_type":"markdown","source":["Running SQL Queries on View"],"metadata":{}},{"cell_type":"code","source":["# load delta format tables from gold bucket\ntest_pred_sp = spark.read.format('delta').load(write_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":80},{"cell_type":"code","source":["# create a temp table\ntable_name = 'test_pred'\ntest_pred_sp.createOrReplaceTempView(table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":81},{"cell_type":"code","source":["%sql\n-- select specific value range\nSELECT *\nFROM test_pred\nWHERE KIRC > 0.6\nORDER BY KIRC DESC"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>PID</th><th>BRCA</th><th>KIRC</th><th>LUAD</th><th>PRAD</th><th>COAD</th></tr></thead><tbody><tr><td>116</td><td>0.0254</td><td>0.9167</td><td>0.0267</td><td>0.0099</td><td>0.0213</td></tr><tr><td>15</td><td>0.0237</td><td>0.916</td><td>0.0248</td><td>0.0116</td><td>0.0239</td></tr><tr><td>232</td><td>0.019</td><td>0.9145</td><td>0.0304</td><td>0.011</td><td>0.0251</td></tr><tr><td>120</td><td>0.0336</td><td>0.9026</td><td>0.0286</td><td>0.0106</td><td>0.0246</td></tr><tr><td>14</td><td>0.0291</td><td>0.9024</td><td>0.0313</td><td>0.0136</td><td>0.0236</td></tr><tr><td>34</td><td>0.032</td><td>0.8996</td><td>0.0356</td><td>0.0132</td><td>0.0197</td></tr><tr><td>87</td><td>0.025</td><td>0.8989</td><td>0.0358</td><td>0.0139</td><td>0.0265</td></tr><tr><td>86</td><td>0.0357</td><td>0.898</td><td>0.0268</td><td>0.0135</td><td>0.0261</td></tr><tr><td>204</td><td>0.0303</td><td>0.8977</td><td>0.0286</td><td>0.017</td><td>0.0264</td></tr><tr><td>96</td><td>0.0306</td><td>0.8945</td><td>0.0366</td><td>0.0144</td><td>0.0238</td></tr><tr><td>136</td><td>0.0319</td><td>0.8922</td><td>0.0339</td><td>0.0151</td><td>0.0268</td></tr><tr><td>21</td><td>0.036</td><td>0.8921</td><td>0.0355</td><td>0.0127</td><td>0.0237</td></tr><tr><td>60</td><td>0.0383</td><td>0.8917</td><td>0.0318</td><td>0.0106</td><td>0.0277</td></tr><tr><td>22</td><td>0.0316</td><td>0.8886</td><td>0.0381</td><td>0.0147</td><td>0.027</td></tr><tr><td>121</td><td>0.0337</td><td>0.8885</td><td>0.0326</td><td>0.0203</td><td>0.0249</td></tr><tr><td>187</td><td>0.0382</td><td>0.8864</td><td>0.0407</td><td>0.0137</td><td>0.0209</td></tr><tr><td>184</td><td>0.0378</td><td>0.8851</td><td>0.0365</td><td>0.0113</td><td>0.0293</td></tr><tr><td>76</td><td>0.0375</td><td>0.8848</td><td>0.0358</td><td>0.0096</td><td>0.0323</td></tr><tr><td>44</td><td>0.0431</td><td>0.8832</td><td>0.0383</td><td>0.0129</td><td>0.0225</td></tr><tr><td>100</td><td>0.032</td><td>0.8831</td><td>0.0477</td><td>0.0121</td><td>0.0252</td></tr><tr><td>134</td><td>0.0335</td><td>0.8817</td><td>0.0411</td><td>0.0161</td><td>0.0277</td></tr><tr><td>180</td><td>0.0399</td><td>0.8805</td><td>0.0413</td><td>0.0117</td><td>0.0266</td></tr><tr><td>177</td><td>0.0394</td><td>0.8769</td><td>0.044</td><td>0.0137</td><td>0.026</td></tr><tr><td>38</td><td>0.0374</td><td>0.8761</td><td>0.0386</td><td>0.0166</td><td>0.0314</td></tr><tr><td>56</td><td>0.0406</td><td>0.8749</td><td>0.0434</td><td>0.0138</td><td>0.0272</td></tr><tr><td>95</td><td>0.0389</td><td>0.8747</td><td>0.0463</td><td>0.0129</td><td>0.0272</td></tr><tr><td>97</td><td>0.0388</td><td>0.8743</td><td>0.0433</td><td>0.0139</td><td>0.0296</td></tr><tr><td>7</td><td>0.0443</td><td>0.8742</td><td>0.0407</td><td>0.0144</td><td>0.0264</td></tr><tr><td>148</td><td>0.046</td><td>0.8731</td><td>0.044</td><td>0.0155</td><td>0.0215</td></tr><tr><td>3</td><td>0.0406</td><td>0.8731</td><td>0.04</td><td>0.0167</td><td>0.0295</td></tr><tr><td>111</td><td>0.044</td><td>0.8731</td><td>0.0361</td><td>0.0146</td><td>0.0322</td></tr><tr><td>158</td><td>0.0451</td><td>0.8715</td><td>0.0453</td><td>0.015</td><td>0.0231</td></tr><tr><td>33</td><td>0.0461</td><td>0.8701</td><td>0.0444</td><td>0.0157</td><td>0.0237</td></tr><tr><td>197</td><td>0.0382</td><td>0.8698</td><td>0.051</td><td>0.0118</td><td>0.0292</td></tr><tr><td>8</td><td>0.0412</td><td>0.8694</td><td>0.0446</td><td>0.0189</td><td>0.026</td></tr><tr><td>62</td><td>0.0443</td><td>0.8666</td><td>0.0393</td><td>0.0175</td><td>0.0322</td></tr><tr><td>84</td><td>0.0477</td><td>0.8659</td><td>0.0418</td><td>0.018</td><td>0.0266</td></tr><tr><td>94</td><td>0.0459</td><td>0.8625</td><td>0.0404</td><td>0.0177</td><td>0.0334</td></tr><tr><td>178</td><td>0.0467</td><td>0.8576</td><td>0.0507</td><td>0.0149</td><td>0.0301</td></tr><tr><td>153</td><td>0.0495</td><td>0.8542</td><td>0.0453</td><td>0.0185</td><td>0.0325</td></tr><tr><td>156</td><td>0.0452</td><td>0.8525</td><td>0.0479</td><td>0.0192</td><td>0.0352</td></tr><tr><td>110</td><td>0.0514</td><td>0.8465</td><td>0.0498</td><td>0.0169</td><td>0.0354</td></tr><tr><td>141</td><td>0.0602</td><td>0.8375</td><td>0.0592</td><td>0.0183</td><td>0.0248</td></tr><tr><td>41</td><td>0.0433</td><td>0.8341</td><td>0.0536</td><td>0.0214</td><td>0.0476</td></tr><tr><td>113</td><td>0.0481</td><td>0.8339</td><td>0.0611</td><td>0.0241</td><td>0.0328</td></tr><tr><td>103</td><td>0.0483</td><td>0.8283</td><td>0.0585</td><td>0.0317</td><td>0.0333</td></tr><tr><td>138</td><td>0.0661</td><td>0.8128</td><td>0.0595</td><td>0.0191</td><td>0.0426</td></tr><tr><td>68</td><td>0.0662</td><td>0.8093</td><td>0.0641</td><td>0.0256</td><td>0.0348</td></tr><tr><td>202</td><td>0.0749</td><td>0.7884</td><td>0.0749</td><td>0.0244</td><td>0.0374</td></tr><tr><td>90</td><td>0.0604</td><td>0.7798</td><td>0.0932</td><td>0.0274</td><td>0.0392</td></tr><tr><td>25</td><td>0.105</td><td>0.7239</td><td>0.099</td><td>0.0279</td><td>0.0442</td></tr><tr><td>131</td><td>0.1449</td><td>0.6814</td><td>0.1128</td><td>0.0277</td><td>0.0333</td></tr></tbody></table></div>"]}}],"execution_count":82},{"cell_type":"markdown","source":["## Traverse Back to Older Timestamp"],"metadata":{}},{"cell_type":"code","source":["# get any version or timestamp\ny_test_pred_1 = spark.read.format(\"delta\") \\\n                          .option(\"timestampAsOf\", \"2020-07-10\") \\\n                          .load(write_location)\n\n# create a temp table\ntable_name = 'test_pred_1'\ny_test_pred_1.createOrReplaceTempView(table_name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":84},{"cell_type":"code","source":["%sql\n-- Different Version of Query on 2020-07-10 00:00:00\nSELECT *\nFROM test_pred_1\nWHERE KIRC > 0.2\nORDER BY KIRC DESC"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>BRCA</th><th>KIRC</th><th>LUAD</th><th>PRAD</th><th>COAD</th></tr></thead><tbody><tr><td>0.2272</td><td>0.2075</td><td>0.1933</td><td>0.1921</td><td>0.1799</td></tr><tr><td>0.2267</td><td>0.2069</td><td>0.1931</td><td>0.1926</td><td>0.1807</td></tr><tr><td>0.2276</td><td>0.2065</td><td>0.1934</td><td>0.192</td><td>0.1806</td></tr><tr><td>0.2264</td><td>0.2064</td><td>0.1933</td><td>0.1925</td><td>0.1813</td></tr><tr><td>0.227</td><td>0.2064</td><td>0.1945</td><td>0.192</td><td>0.1801</td></tr><tr><td>0.2266</td><td>0.2063</td><td>0.1933</td><td>0.1929</td><td>0.1809</td></tr><tr><td>0.2258</td><td>0.2063</td><td>0.1933</td><td>0.1932</td><td>0.1813</td></tr><tr><td>0.2268</td><td>0.2063</td><td>0.1938</td><td>0.1929</td><td>0.1802</td></tr><tr><td>0.2275</td><td>0.2062</td><td>0.1944</td><td>0.1909</td><td>0.1809</td></tr><tr><td>0.2269</td><td>0.2062</td><td>0.1941</td><td>0.1919</td><td>0.1809</td></tr><tr><td>0.2266</td><td>0.2062</td><td>0.194</td><td>0.1924</td><td>0.1808</td></tr><tr><td>0.2269</td><td>0.2061</td><td>0.1941</td><td>0.1922</td><td>0.1807</td></tr><tr><td>0.2259</td><td>0.2061</td><td>0.194</td><td>0.1927</td><td>0.1813</td></tr><tr><td>0.2255</td><td>0.206</td><td>0.1941</td><td>0.1932</td><td>0.1812</td></tr><tr><td>0.2262</td><td>0.2059</td><td>0.1936</td><td>0.1934</td><td>0.1809</td></tr><tr><td>0.2259</td><td>0.2059</td><td>0.1939</td><td>0.193</td><td>0.1813</td></tr><tr><td>0.2269</td><td>0.2059</td><td>0.194</td><td>0.1921</td><td>0.1811</td></tr><tr><td>0.2259</td><td>0.2059</td><td>0.1936</td><td>0.1929</td><td>0.1817</td></tr><tr><td>0.227</td><td>0.2057</td><td>0.1936</td><td>0.1927</td><td>0.1811</td></tr><tr><td>0.2265</td><td>0.2055</td><td>0.1938</td><td>0.1928</td><td>0.1814</td></tr><tr><td>0.2277</td><td>0.2055</td><td>0.1944</td><td>0.1925</td><td>0.18</td></tr><tr><td>0.2271</td><td>0.2054</td><td>0.1931</td><td>0.1935</td><td>0.181</td></tr><tr><td>0.2283</td><td>0.2053</td><td>0.1944</td><td>0.192</td><td>0.1801</td></tr><tr><td>0.2275</td><td>0.2053</td><td>0.1943</td><td>0.1923</td><td>0.1806</td></tr><tr><td>0.2267</td><td>0.2053</td><td>0.1946</td><td>0.1926</td><td>0.1807</td></tr><tr><td>0.2271</td><td>0.2052</td><td>0.1938</td><td>0.1931</td><td>0.1807</td></tr><tr><td>0.2268</td><td>0.2052</td><td>0.1942</td><td>0.1927</td><td>0.1812</td></tr><tr><td>0.2271</td><td>0.2051</td><td>0.194</td><td>0.1926</td><td>0.1813</td></tr><tr><td>0.2276</td><td>0.205</td><td>0.1931</td><td>0.1936</td><td>0.1806</td></tr><tr><td>0.2281</td><td>0.2049</td><td>0.1929</td><td>0.1931</td><td>0.181</td></tr><tr><td>0.228</td><td>0.2049</td><td>0.1933</td><td>0.1928</td><td>0.181</td></tr><tr><td>0.2263</td><td>0.2047</td><td>0.1947</td><td>0.193</td><td>0.1813</td></tr><tr><td>0.2286</td><td>0.2046</td><td>0.195</td><td>0.192</td><td>0.1798</td></tr><tr><td>0.228</td><td>0.2043</td><td>0.1945</td><td>0.1926</td><td>0.1806</td></tr><tr><td>0.227</td><td>0.2035</td><td>0.1948</td><td>0.1932</td><td>0.1815</td></tr><tr><td>0.2275</td><td>0.2033</td><td>0.1951</td><td>0.1929</td><td>0.1812</td></tr><tr><td>0.2304</td><td>0.203</td><td>0.1942</td><td>0.193</td><td>0.1794</td></tr><tr><td>0.2285</td><td>0.203</td><td>0.1951</td><td>0.1922</td><td>0.1812</td></tr><tr><td>0.2272</td><td>0.2022</td><td>0.1942</td><td>0.1957</td><td>0.1807</td></tr><tr><td>0.2284</td><td>0.2016</td><td>0.1968</td><td>0.1929</td><td>0.1804</td></tr><tr><td>0.2309</td><td>0.2015</td><td>0.1954</td><td>0.1923</td><td>0.1799</td></tr><tr><td>0.2284</td><td>0.2014</td><td>0.1973</td><td>0.1932</td><td>0.1797</td></tr><tr><td>0.2301</td><td>0.2014</td><td>0.1947</td><td>0.1929</td><td>0.1809</td></tr><tr><td>0.2281</td><td>0.2013</td><td>0.1958</td><td>0.1929</td><td>0.1819</td></tr><tr><td>0.2283</td><td>0.2012</td><td>0.1978</td><td>0.1921</td><td>0.1806</td></tr><tr><td>0.2281</td><td>0.2011</td><td>0.1969</td><td>0.1928</td><td>0.1811</td></tr><tr><td>0.2294</td><td>0.201</td><td>0.1951</td><td>0.1937</td><td>0.1807</td></tr><tr><td>0.2314</td><td>0.201</td><td>0.1952</td><td>0.1924</td><td>0.1801</td></tr><tr><td>0.2296</td><td>0.201</td><td>0.1961</td><td>0.1924</td><td>0.1809</td></tr><tr><td>0.2308</td><td>0.2008</td><td>0.1935</td><td>0.1947</td><td>0.1801</td></tr><tr><td>0.2292</td><td>0.2007</td><td>0.1958</td><td>0.1931</td><td>0.1813</td></tr><tr><td>0.2289</td><td>0.2005</td><td>0.1927</td><td>0.1978</td><td>0.1801</td></tr><tr><td>0.23</td><td>0.2005</td><td>0.1947</td><td>0.1935</td><td>0.1813</td></tr><tr><td>0.23</td><td>0.2004</td><td>0.1952</td><td>0.1929</td><td>0.1815</td></tr><tr><td>0.2279</td><td>0.2004</td><td>0.1983</td><td>0.1917</td><td>0.1817</td></tr><tr><td>0.2287</td><td>0.2003</td><td>0.1978</td><td>0.1921</td><td>0.181</td></tr><tr><td>0.2278</td><td>0.2003</td><td>0.198</td><td>0.1931</td><td>0.1808</td></tr><tr><td>0.2289</td><td>0.2003</td><td>0.196</td><td>0.1933</td><td>0.1815</td></tr><tr><td>0.2301</td><td>0.2002</td><td>0.1957</td><td>0.193</td><td>0.181</td></tr><tr><td>0.2281</td><td>0.2002</td><td>0.199</td><td>0.191</td><td>0.1817</td></tr><tr><td>0.2283</td><td>0.2001</td><td>0.1973</td><td>0.1931</td><td>0.1812</td></tr><tr><td>0.2303</td><td>0.2001</td><td>0.1953</td><td>0.1938</td><td>0.1806</td></tr><tr><td>0.2307</td><td>0.2001</td><td>0.1939</td><td>0.1943</td><td>0.1811</td></tr><tr><td>0.2319</td><td>0.2001</td><td>0.194</td><td>0.1926</td><td>0.1815</td></tr><tr><td>0.2289</td><td>0.2001</td><td>0.1975</td><td>0.1925</td><td>0.1811</td></tr><tr><td>0.2284</td><td>0.2001</td><td>0.1966</td><td>0.1931</td><td>0.1818</td></tr><tr><td>0.227</td><td>0.2</td><td>0.1992</td><td>0.1924</td><td>0.1813</td></tr><tr><td>0.2301</td><td>0.2</td><td>0.1963</td><td>0.1924</td><td>0.1812</td></tr><tr><td>0.2278</td><td>0.2</td><td>0.1936</td><td>0.1976</td><td>0.181</td></tr><tr><td>0.2313</td><td>0.2</td><td>0.1934</td><td>0.1938</td><td>0.1815</td></tr><tr><td>0.2308</td><td>0.2</td><td>0.1951</td><td>0.1933</td><td>0.1808</td></tr><tr><td>0.2298</td><td>0.2</td><td>0.1956</td><td>0.1932</td><td>0.1814</td></tr><tr><td>0.2303</td><td>0.2</td><td>0.1955</td><td>0.1925</td><td>0.1817</td></tr></tbody></table></div>"]}}],"execution_count":85}],"metadata":{"name":"Tumor Prediction_Delta_ML","notebookId":1522018663773981},"nbformat":4,"nbformat_minor":0}
